---
title: "ds4002jterm"
author: "Allison Fowle"
date: "2023-01-03"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Read in Libraries
```{r}
library(tidyverse)
library(caret)
library(ROCR)
library(MLmetrics)
library(mltools)
library(rpart.plot)
library(dplyr)
library(DT)
library(randomForest)
library(rio)
library(class)
library(plotly)
library(mice)
library(data.table)
library(RColorBrewer)
```

# Read in Data
```{r}
philly <- read.csv("C:/Users/allis/Documents/UVA-January 2023/Properties_philly_Kraggle_v2.csv")
```

# Data cleaning
```{r}
# Drop address, zillow address, sale date, opening bid, book writ, OPA, attorney, ward, seller, buyer, other, record deed, zillow/rent estimate, tax assessment
column_index <- tibble(colnames(philly)) 
philly <- philly[,-c(1:4, 6, 7, 9:12, 15, 16, 22:24, 30)]


# rename target variable from sale.price.bid.price
colnames(philly)[1] = "Sale.Price"
philly$Sale.Price <- gsub(',','',philly$Sale.Price)
philly$Sale.Price <- as.numeric(gsub('[$,]', '', philly$Sale.Price))

#substituting characters with NA to use as factors
philly$bathrooms <- gsub("-", "NA", philly$bathrooms)
philly$bedrooms <- gsub("-", "NA", philly$bedrooms)
philly[,c(12, 13, 14)] <- lapply(philly[,c(12, 13, 14)], as.factor)

```

# Data visualization/Summary Statistics
```{r}
# We still want to see some helpful visualizations, so we'll use a dataset that hasn't been cleaned yet 
scat <- read.csv("C:/Users/allis/Documents/UVA-January 2023/Properties_philly_Kraggle_v2.csv")
plot(scat$bedrooms, scat$bathrooms, pch = 19, col = "blue", xlab = "Number of Beds", ylab = "Number of Baths", main = "Scatterplot of Beds and Baths in Philedelphia, PA")

# for each zip code find the mean price
philly %>% group_by(Postal.Code) %>% 
  summarise(avg_price=mean(Sale.Price),
            .groups = 'drop')

```

# decision tree 80/10/10
```{r}
set.seed(1)
philly <- philly[complete.cases(philly), ]
# that took away 200 of our records

abc <- names(select_if(philly, is.factor))
philly_1h <- one_hot(as.data.table(philly),cols=abc,sparsifyNAs = TRUE,naCols = FALSE,dropCols = TRUE,dropUnusedLevels = TRUE)


#ds1 indicates the dataset that has not been one-hot encoded. ds2 is one-hot encoded
set.seed(1)
part_index_ds1 <- caret::createDataPartition(philly$Sale.Price,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)
part_index_ds2 <- caret::createDataPartition(philly_1h$Sale.Price,
                                           times=1,
                                           p = 0.80,
                                           groups=1,
                                           list=FALSE)

train_ds1 <- philly[part_index_ds1, ]
train_ds2 <- philly_1h[part_index_ds2, ]
tune_and_test_ds1 <- philly[-part_index_ds1, ]
tune_and_test_ds2<- philly_1h[-part_index_ds2, ]

#The we need to use the function again to create the tuning set 
tune_and_test_index_ds1 <- createDataPartition(tune_and_test_ds1$Sale.Price,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)
tune_and_test_index_ds2 <- createDataPartition(tune_and_test_ds2$Sale.Price,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune_ds1 <- tune_and_test_ds1[tune_and_test_index_ds1, ]
tune_ds2 <- tune_and_test_ds2[tune_and_test_index_ds2, ]
test_ds1 <- tune_and_test_ds1[-tune_and_test_index_ds1, ]
test_ds2 <- tune_and_test_ds2[-tune_and_test_index_ds2, ]


## 5. Build your model using the training data, rpart2, and repeated cross validation as reviewed in class with the caret package.

# Choose the features and classes, slightly different approach for caret, need to create features and target sets from the training data.

features_ds1 <- train_ds1[,-1]#dropping 1 because it's target variable (price)
features_ds2 <- train_ds2[,-1]
target_ds1 <- data_frame(price=train_ds1$Sale.Price)
target_ds2 <- data_frame(price=train_ds2$Sale.Price)


#Step 1: Cross validation process-the process by which the training data will be used to build the initial model must be set. As seen below:

fitControl <- trainControl(method = "repeatedcv",
                          number = 10,
                          repeats = 10) 

# number - number of folds
# repeats - number of times the CV is repeated, takes the average of these repeat rounds

#Step 2: Usually involves setting a hyper-parameter search. This is optional and the hyper-parameters vary by model. Let's take a look at the documentation for the model we are going to use. Same search function as for classification 

tree.grid <- expand.grid(maxdepth=c(3:10))

#  2^(k+1)âˆ’1 = maximum number of nodes (splits) when k=depth of the 
str(features_ds1)

#Step 3: Train the models
set.seed(1)
ds1_mdl_r <- train(x=features_ds1,
                y=target_ds1$price,
                method="rpart2",
                trControl=fitControl,
                metric="RMSE")
ds1_mdl_r
ds2_mdl_r <- train(x=features_ds2,
                y=target_ds2$price,
                method="rpart2",
                trControl=fitControl,
                metric="RMSE")
ds2_mdl_r

ds1_mdl_1_r <- train(x=features_ds1,
                y=target_ds1$price,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid,#expanded grid
                metric="RMSE")

ds1_mdl_1_r
varImp(ds1_mdl_1_r)

ds2_mdl_2_r <- train(x=features_ds2,
                y=target_ds2$price,
                method="rpart2",#type of model uses maxdepth to select a model
                trControl=fitControl,#previously created
                tuneGrid=tree.grid,#expanded grid
                metric="RMSE")

ds2_mdl_2_r
varImp(ds2_mdl_2_r)

# we should put var imp in table format and threshold metrics
rpart.plot(ds1_mdl_1_r$finalModel, type=4,extra=101)
rpart.plot(ds2_mdl_2_r$finalModel, type=4,extra=101)


#Finds RMSE of our prediction vs actual
#predicted dataset without 1h
pred_tune <- predict(ds1_mdl_1_r, tune_ds1)
postResample(pred = pred_tune, obs = tune_ds1$Sale.Price)
#NRMSE
range(tune_ds1$Sale.Price)
185000-5900
11085.12/179100
# NRMSE: 0.0619 --> 6.19%

#predicted dataset with 1h
pred_tune2 <- predict(ds2_mdl_2_r, tune_ds2)
postResample(pred = pred_tune2, obs = tune_ds2$Sale.Price)
#NRMSE
range(tune_ds2$Sale.Price)
159000-5200
13810.28/153800
# NRMSE: 0.08979 --> 8.98%


# actual dataset
pred_test <- predict(ds1_mdl_1_r, test_ds1)
postResample(pred = pred_test, obs = test_ds1$Sale.Price)
#NRMSE
range(test_ds1$Sale.Price)
233000-6700
11685.79/226300
# NRMSE: 0.0516 --> 5.16%


# Dataframe for tree without expanded grid, comparing 1h vs normal

Max_Depth <- c('1', '2', '3')
RMSE <- c('28601.45', '27249.34', '13714.48')
RMSE2 <- c('28083.39', '23938.40', '12542.03')
Rsquared <- c('0.6785758', '0.7062375', '0.9280697')
Rsquared2 <- c('0.6966192', '0.7822921', '0.9387921')
df1 <- data.frame(Max_Depth, RMSE, RMSE2, Rsquared, Rsquared2)
df1

# EXPANDED
Max_Depth_e <- c('3', '4', '5', '6', '7', '8', '9', '10')
RMSE_e <- c('19007.68', '16064.10', '14461.09', '13407.70', '13352.81', '13352.81', '13352.81', '13352.81')
RMSE2_e <- c('18821.43', '15604.80', '13384.98', '12929.44', '12876.00', '12876.00', '12876.00', '12876.00')
Rsquared_e <- c('0.8650238', '0.9001380', '0.9198414', '0.9313778', '0.9313778', '0.9313778', '0.9313778', '0.9313778')
Rsquared2_e <- c('0.8649066', '0.9057865', '0.9316122', '0.9360844', '0.9364067', '0.9364067', '0.9364067', '0.9364067')
df2 <- data.frame(Max_Depth_e, RMSE_e, RMSE2_e, Rsquared_e, Rsquared2_e)
df2
```



```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}

mytry_tune(philly)
# 3.6
mytry_tune(philly_1h)
# 3.7


# --------------------------------------STARTING RANDOM FOREST ---------------------------------------
str(train_ds1)
set.seed(99)	
philly_RF = randomForest(Sale.Price~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            data=train_ds1,     #<- A data frame with the variables to be used.
                            #y = train_ds1$Sale.Price,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = preds,       #<- This is already defined in the formula by the ".".
                            #ytest = census_train$income,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 15,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 




philly_RF1h = randomForest(Sale.Price~.,          #<- Formula: response variable ~ predictors.
                            #   The period means 'use all other variables in the data'.
                            data=train_ds2,     #<- A data frame with the variables to be used.
                            #y = train_ds1$Sale.Price,           #<- A response vector. This is unnecessary because we're specifying a response formula.
                            #subset = NULL,      #<- This is unnecessary because we're using all the rows in the training data set.
                            #xtest = preds,       #<- This is already defined in the formula by the ".".
                            #ytest = census_train$income,       #<- This is already defined in the formula by "PREGNANT".
                            ntree = 500,        #<- Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets classified at least a few times.
                            mtry = 15,            #<- Number of variables randomly sampled as candidates at each split. Default number for classification is sqrt(# of variables). Default number for regression is (# of variables / 3).
                            replace = TRUE,      #<- Should sampled data points be replaced.
                            #classwt = NULL,     #<- Priors of the classes. Use this if you want to specify what proportion of the data SHOULD be in each class. This is relevant if your sample data is not completely representative of the actual population 
                            #strata = NULL,      #<- Not necessary for our purpose here.
                            sampsize = 100,      #<- Size of sample to draw each time.
                            nodesize = 5,        #<- Minimum numbers of data points in terminal nodes.
                            #maxnodes = NULL,    #<- Limits the number of maximum splits. 
                            importance = TRUE,   #<- Should importance of predictors be assessed?
                            #localImp = FALSE,   #<- Should casewise importance measure be computed? (Setting this to TRUE will override importance.)
                            proximity = FALSE,    #<- Should a proximity measure between rows be calculated?
                            norm.votes = TRUE,   #<- If TRUE (default), the final result of votes are expressed as fractions. If FALSE, raw vote counts are returned (useful for combining results from different runs).
                            do.trace = TRUE,     #<- If set to TRUE, give a more verbose output as randomForest is run.
                            keep.forest = TRUE,  #<- If set to FALSE, the forest will not be retained in the output object. If xtest is given, defaults to FALSE.
                            keep.inbag = TRUE)   #<- Should an n by ntree matrix be returned that keeps track of which samples are in-bag in which trees? 


#==================================================================================

# Look at the output of the random forest.
philly_RF
#% Var explained: 94.44

philly_RF1h
#% Var explained: 95.47

```





```{r}
ggplot(philly, aes(x=Sheriff.Cost, y=Sale.Price)) +
  geom_point(size=2, shape=20, color='red') +
  geom_smooth(method=lm, se=FALSE) +
  labs(title = 'Correlation Between Sale Price & Sherriff Cost in Philadelphia') +
  xlab('Sherriff Cost') +
  ylab('Sale Price') +
  ylim(-10000, 325000)


vi <- varImp(ds1_mdl_1_r)
vi
typeof(vi)
vi2 <- do.call(rbind.data.frame, vi)
vi2 <- cbind(variable = rownames(vi2), vi2)
rownames(vi2) <- 1:nrow(vi2)
vi2 <- vi2[-c(14,15),]
vi2$Overall <- as.numeric(vi2$Overall)
vi2$variable <- gsub('importance.', '', vi2$variable)
vi2$Overall <- vi2[order(-vi2$Overall),]

ggplot(vi2, aes(x=Overall$variable, y=Overall$Overall)) +
  geom_bar(stat='identity', color="purple") +
  labs(title ="Variable Importance of Original Dataset") +
  ylab("Importance Percentage") +
  xlab("Variable") +
  theme_classic()+
  coord_flip()


```


we changed tree value from 500 to 100